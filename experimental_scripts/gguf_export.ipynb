{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3553bd73-ec5d-4067-9cd1-3940e224dc60",
      "metadata": {
        "id": "3553bd73-ec5d-4067-9cd1-3940e224dc60"
      },
      "source": [
        "# Export your Fine-Tuned Model to GGUF to Run Locally ðŸ¤™\n",
        "\n",
        "Welcome!\n",
        "\n",
        "We will export a checkpoint from our fine-tuned model ([Fine-tune Mistral 7B on your own data](), [Fine-tune Mistral 7B on HF dataset](), [Fine-tune Llama 2 on your own data]()) to a GGUF (the updated version of GGML) file. This will allow you to run your model locally, on your CPU, and/or on any GPUs your machine may have. You can also upload it to local LLM UI applications like [Ollama](https://ollama.ai/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1354c7-c117-489d-897e-abb989f88f14",
      "metadata": {
        "id": "8c1354c7-c117-489d-897e-abb989f88f14"
      },
      "source": [
        "## Let's begin!\n",
        "\n",
        "\n",
        "Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used Python 3.10 and CUDA 12.0.1; these should be preconfigured for you if you use the badge above) and click the \"Build\" button to build your verb container. Give this a few minutes.\n",
        "\n",
        "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
        "\n",
        "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7245daac-c4ed-4d52-8de6-5376232bdf98",
      "metadata": {
        "id": "7245daac-c4ed-4d52-8de6-5376232bdf98"
      },
      "source": [
        "## 1. Save Model, Tokenizer, and LoRA Checkpoint\n",
        "First, we want to create a script to save the model, tokenizer, and LoRA checkpoint (which you have uploaded) to a file that we can then transform to GGUF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff85a597-a285-418c-b453-d133e92dfdc4",
      "metadata": {
        "scrolled": true,
        "id": "ff85a597-a285-418c-b453-d133e92dfdc4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch torchvision torchaudio peft accelerate sentencepiece gguf\n",
        "!pip install -U git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bcd661e-86ed-468c-b1cc-a995d491c9c4",
      "metadata": {
        "id": "0bcd661e-86ed-468c-b1cc-a995d491c9c4",
        "outputId": "c5c9ef3b-c4ff-4fe0-f76c-4f9867f033d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting save_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile save_model.py\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str)\n",
        "    parser.add_argument(\"--lora\", type=str)\n",
        "    parser.add_argument(\"--out_dir\", type=str, default=\"./model\") # leave this\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Loading base model: {args.model}\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(args.model, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "    print(f\"Loading PEFT: {args.lora}\")\n",
        "    model = PeftModel.from_pretrained(base_model, args.lora)\n",
        "    print(f\"Running merge_and_unload\")\n",
        "    model = model.merge_and_unload()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
        "\n",
        "    model.save_pretrained(f\"{args.out_dir}\")\n",
        "    tokenizer.save_pretrained(f\"{args.out_dir}\")\n",
        "    print(f\"Model saved to {args.out_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1920253-4d06-41bd-ab72-4d1af423ef61",
      "metadata": {
        "id": "e1920253-4d06-41bd-ab72-4d1af423ef61"
      },
      "source": [
        "Next, we run the script. Replace `mistralai/Mistral-7B-v0.1` and `checkpoint-500` below with the names of the correct Hugging Face base model, and your appropriate checkpoint number. Recall the `checkpoint-[STEP#]` directory should look like this:\n",
        "\n",
        "[link text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DdHAG77ErS3m",
        "outputId": "e7582718-ffdd-431e-c0fe-953e1864adcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model: mistralai/Mistral-7B-v0.1\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:58<00:00, 59.32s/it]\n",
            "Loading PEFT: checkpoint-500\n",
            "Running merge_and_unload\n",
            "Model saved to ./model\n"
          ]
        }
      ],
      "source": [
        "!python save_model.py --model 'mistralai/Mistral-7B-v0.1' --lora 'checkpoint-500'"
      ],
      "id": "DdHAG77ErS3m"
    },
    {
      "cell_type": "markdown",
      "id": "c4e580b8-f10d-493b-8b36-45125123cfd5",
      "metadata": {
        "id": "c4e580b8-f10d-493b-8b36-45125123cfd5"
      },
      "source": [
        "## 2. Convert to GGUF\n",
        "Now we convert to GGUF using the `llama.cpp` `convert.py` script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119c11a7-26dd-4e3b-890b-4aa912d8a204",
      "metadata": {
        "id": "119c11a7-26dd-4e3b-890b-4aa912d8a204"
      },
      "outputs": [],
      "source": [
        "!curl -L -o convert.py https://github.com/ggerganov/llama.cpp/raw/master/convert.py\n",
        "!curl -L -o requirements.txt https://github.com/ggerganov/llama.cpp/raw/master/requirements.txt\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Convert the 7B model to ggml FP16 format\n",
        "!python3 convert.py model\n",
        "\n",
        "## Code below is optional - uncomment (remove leftmost '# ') to use\n",
        "\n",
        "# # [Optional] for models using BPE tokenizers\n",
        "# !python convert.py model --vocabtype bpe\n",
        "\n",
        "# # [Optional] quantize the model to 4-bits (using q4_0 method)\n",
        "# !./quantize ./model/ggml-model-f16.gguf ./model/ggml-model-q4_0.gguf q4_0\n",
        "\n",
        "# # [Optional] update the gguf filetype to current if older version is unsupported by another application\n",
        "# !./quantize ./model/ggml-model-q4_0.gguf ./model/ggml-model-q4_0-v2.gguf COPY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833bdbd6-7699-4de9-9cf5-a20cd2abfe61",
      "metadata": {
        "id": "833bdbd6-7699-4de9-9cf5-a20cd2abfe61"
      },
      "source": [
        "### Excellent! Now you have the GGUF file.\n",
        "This can be uploaded to Oobabooga (an LLM user interface, akin to Stable Diffusion's AUTOMATIC1111) - see tutorial [here](https://github.com/brevdev/notebooks/blob/main/oobabooga.ipynb) - or you can run from the command line using [Ollama](https://ollama.ai/). For Ollama instructions, click [here](https://github.com/jmorganca/ollama#customize-your-own-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee27a9c-b1aa-460a-8e04-437f7ea19f36",
      "metadata": {
        "id": "eee27a9c-b1aa-460a-8e04-437f7ea19f36"
      },
      "source": [
        "## 3. Optional: Save to your Local Machine\n",
        "\n",
        "If you'd like to download the model to your local machine, open up your favorite shell on your local machine (or choose the application \"Terminal\" if you're not familiar) and enter the command:\n",
        "\n",
        "`scp ubuntu@export-ggml:~/model/ggml-model-f16.gguf ~/Desktop`\n",
        "\n",
        "This assumes an instance name `export-ggml`, the gguf file is named `ggml-model-f16.gguf` and it is in a `model` directory within your home directory, and you'd like to save it to your local Desktop. **This download will take a while, probably a few hours.**\n",
        "\n",
        "Great! Now you've downloaded the `gguf` file for your fine-tuned model, which you can run locally on your machine. Again, services like [Ollama](https://ollama.ai/) are great for this.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}